'use client';

import { useCallback, useRef, useState, useEffect } from 'react';
import { useTranscriptStore, TranscriptEntry } from '@/stores';

interface UseWebSpeechSTTOptions {
  speaker: 'customer' | 'counselor';
  language?: string;
  continuous?: boolean;
  interimResults?: boolean;
}

export function useWebSpeechSTT(options: UseWebSpeechSTTOptions) {
  const {
    speaker,
    language = 'ko-KR',
    continuous = true,
    interimResults = true,
  } = options;

  const [isListening, setIsListening] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [isSupported, setIsSupported] = useState(true);

  const recognitionRef = useRef<any>(null);
  const interimTranscriptRef = useRef<string>('');
  const finalTranscriptRef = useRef<string>('');

  const { addTranscript, updateInterim, setStreaming } = useTranscriptStore();

  // ë¸Œë¼ìš°ì € ì§€ì› í™•ì¸ ë° ì´ˆê¸°í™”
  useEffect(() => {
    const SpeechRecognition =
      (window as any).SpeechRecognition ||
      (window as any).webkitSpeechRecognition;

    if (!SpeechRecognition) {
      setIsSupported(false);
      setError('ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. Chromeì„ ì‚¬ìš©í•´ì£¼ì„¸ìš”.');
      return;
    }

    console.log('ðŸŽ¤ Initializing Web Speech API for:', speaker);

    // Speech Recognition ì´ˆê¸°í™”
    const recognition = new SpeechRecognition();
    recognition.lang = language;
    recognition.continuous = continuous;
    recognition.interimResults = interimResults;
    recognition.maxAlternatives = 1;

    console.log('ðŸ“‹ Speech Recognition config:', {
      lang: language,
      continuous,
      interimResults,
      speaker,
    });

    // ê²°ê³¼ ì´ë²¤íŠ¸
    recognition.onresult = (event: any) => {
      let interimTranscript = '';
      let finalTranscript = '';

      console.log('Speech recognition result:', event.results);

      // ëª¨ë“  ê²°ê³¼ ì²˜ë¦¬
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;

        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }

      // ì‹¤ì‹œê°„ ìžë§‰ (ì¤‘ê°„ ê²°ê³¼) - ë§¤ë²ˆ ìƒˆë¡œìš´ ê°ì²´ ìƒì„±
      if (interimTranscript) {
        console.log(`[${speaker}] Interim:`, interimTranscript);
        interimTranscriptRef.current = interimTranscript;
        const entry: TranscriptEntry = {
          id: `interim-${speaker}-${Date.now()}`, // ë§¤ë²ˆ ìƒˆë¡œìš´ ID
          speaker,
          text: interimTranscript,
          isFinal: false,
          timestamp: new Date(),
        };
        updateInterim(entry);
      }

      // ìµœì¢… ìžë§‰
      if (finalTranscript) {
        console.log(`[${speaker}] Final:`, finalTranscript);
        finalTranscriptRef.current += finalTranscript;
        const entry: TranscriptEntry = {
          id: `transcript-${Date.now()}`,
          speaker,
          text: finalTranscript.trim(),
          isFinal: true,
          timestamp: new Date(),
        };
        addTranscript(entry);

        // ì¤‘ê°„ ê²°ê³¼ ì´ˆê¸°í™”
        interimTranscriptRef.current = '';
      }
    };

    // ìŒì„± ìž…ë ¥ ì‹œìž‘
    recognition.onstart = () => {
      console.log('Speech recognition started');
      setIsListening(true);
      setStreaming(true);
      setError(null);
    };

    // ìŒì„± ìž…ë ¥ ì¢…ë£Œ
    recognition.onend = () => {
      console.log('Speech recognition ended');

      // continuous ëª¨ë“œì´ê³  ì—¬ì „ížˆ listening ìƒíƒœë©´ ìžë™ ìž¬ì‹œìž‘
      if (continuous && isListening) {
        try {
          recognition.start();
        } catch (err) {
          console.log('Recognition already started or ended');
        }
      } else {
        setIsListening(false);
        setStreaming(false);
      }
    };

    // ì—ëŸ¬ ì²˜ë¦¬
    recognition.onerror = (event: any) => {
      console.error('Speech recognition error:', event.error);

      // ì¼ë¶€ ì—ëŸ¬ëŠ” ë¬´ì‹œ (no-speechëŠ” ì •ìƒ)
      if (event.error === 'no-speech') {
        return;
      }

      if (event.error === 'aborted') {
        return;
      }

      setError(`ìŒì„± ì¸ì‹ ì˜¤ë¥˜: ${event.error}`);

      // ê¶Œí•œ ê±°ë¶€ ë“±ì˜ ì¹˜ëª…ì  ì—ëŸ¬ëŠ” ì¤‘ì§€
      if (event.error === 'not-allowed' || event.error === 'service-not-allowed') {
        setIsListening(false);
        setStreaming(false);
      }
    };

    // ìŒì„± ìž…ë ¥ ì‹œìž‘ ê°ì§€
    recognition.onspeechstart = () => {
      console.log('Speech started');
    };

    // ìŒì„± ìž…ë ¥ ì¢…ë£Œ ê°ì§€
    recognition.onspeechend = () => {
      console.log('Speech ended');
    };

    // ì˜¤ë””ì˜¤ ìº¡ì²˜ ì‹œìž‘
    recognition.onaudiostart = () => {
      console.log('Audio capturing started');
    };

    // ì˜¤ë””ì˜¤ ìº¡ì²˜ ì¢…ë£Œ
    recognition.onaudioend = () => {
      console.log('Audio capturing ended');
    };

    recognitionRef.current = recognition;

    return () => {
      if (recognitionRef.current) {
        try {
          recognitionRef.current.stop();
        } catch (err) {
          console.log('Recognition cleanup error (safe to ignore):', err);
        }
      }
    };
  }, [language, continuous, interimResults, speaker, addTranscript, updateInterim, setStreaming]);

  // ìŒì„± ì¸ì‹ ì‹œìž‘
  const startListening = useCallback(() => {
    if (!isSupported) {
      setError('ìŒì„± ì¸ì‹ì´ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.');
      return;
    }

    if (!recognitionRef.current) {
      setError('ìŒì„± ì¸ì‹ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
      return;
    }

    try {
      interimTranscriptRef.current = '';
      finalTranscriptRef.current = '';
      recognitionRef.current.start();
    } catch (err) {
      console.error('Failed to start recognition:', err);
      // ì´ë¯¸ ì‹œìž‘ëœ ê²½ìš° ë¬´ì‹œ
      if (err instanceof Error && err.message.includes('already started')) {
        setIsListening(true);
        setStreaming(true);
      } else {
        setError('ìŒì„± ì¸ì‹ì„ ì‹œìž‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
      }
    }
  }, [isSupported, setStreaming]);

  // ìŒì„± ì¸ì‹ ì¤‘ì§€
  const stopListening = useCallback(() => {
    if (recognitionRef.current) {
      try {
        recognitionRef.current.stop();
        setIsListening(false);
        setStreaming(false);
      } catch (err) {
        console.error('Failed to stop recognition:', err);
      }
    }
  }, [setStreaming]);

  return {
    isListening,
    isSupported,
    error,
    startListening,
    stopListening,
  };
}
