'use client';

import { useCallback, useRef, useState, useEffect } from 'react';
import { useTranscriptStore, TranscriptEntry } from '@/stores';

interface UseRealtimeSTTFixedOptions {
  speaker: 'customer' | 'counselor';
}

export function useRealtimeSTTFixed(options: UseRealtimeSTTFixedOptions) {
  const { speaker } = options;

  const [isConnected, setIsConnected] = useState(false);
  const [isConnecting, setIsConnecting] = useState(false);
  const [error, setError] = useState<string | null>(null);

  const peerConnectionRef = useRef<RTCPeerConnection | null>(null);
  const dataChannelRef = useRef<RTCDataChannel | null>(null);
  const audioElementRef = useRef<HTMLAudioElement | null>(null);
  const currentSegmentIdRef = useRef<string>('');

  const { addTranscript, updateInterim, setStreaming } = useTranscriptStore();

  // ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
  const handleServerEvent = useCallback((event: MessageEvent) => {
    try {
      const serverEvent = JSON.parse(event.data);
      console.log(`[${speaker}] ðŸ“¨ Server event:`, serverEvent.type, serverEvent);

      // ì„¸ì…˜ ìƒì„±
      if (serverEvent.type === 'session.created') {
        console.log(`[${speaker}] âœ… Session created`);
      }

      // ì„¸ì…˜ ì—…ë°ì´íŠ¸ ì™„ë£Œ
      if (serverEvent.type === 'session.updated') {
        console.log(`[${speaker}] âœ… Session updated - transcription enabled`);
      }

      // ìŒì„± ìž…ë ¥ ì‹œìž‘
      if (serverEvent.type === 'input_audio_buffer.speech_started') {
        console.log(`[${speaker}] ðŸ—£ï¸ Speech started`);
        // ìƒˆë¡œìš´ ì„¸ê·¸ë¨¼íŠ¸ ì‹œìž‘
        currentSegmentIdRef.current = `${speaker}-${Date.now()}`;
      }

      // ìŒì„± ìž…ë ¥ ì¤‘ì§€
      if (serverEvent.type === 'input_audio_buffer.speech_stopped') {
        console.log(`[${speaker}] ðŸ¤ Speech stopped`);
      }

      // ðŸ”¥ í•µì‹¬: ì „ì‚¬ ì§„í–‰ ì¤‘ (ì‹¤ì‹œê°„ partial)
      if (serverEvent.type === 'conversation.item.input_audio_transcription.delta') {
        const delta = serverEvent.delta;
        if (delta) {
          console.log(`[${speaker}] â³ INTERIM delta:`, delta);

          // ìž„ì‹œ ìžë§‰ ì—…ë°ì´íŠ¸
          const entry: TranscriptEntry = {
            id: currentSegmentIdRef.current,
            speaker,
            text: delta,
            isFinal: false,
            timestamp: new Date(),
          };
          updateInterim(entry);
        }
      }

      // ðŸ”¥ í•µì‹¬: ì „ì‚¬ ì™„ë£Œ (ìµœì¢… í™•ì •)
      if (serverEvent.type === 'conversation.item.input_audio_transcription.completed') {
        const transcript = serverEvent.transcript;
        if (transcript && transcript.trim()) {
          console.log(`[${speaker}] âœ… FINAL transcript:`, transcript);

          // ìµœì¢… ìžë§‰ ì €ìž¥
          const entry: TranscriptEntry = {
            id: `transcript-${Date.now()}`,
            speaker,
            text: transcript.trim(),
            isFinal: true,
            timestamp: new Date(),
          };
          addTranscript(entry);
        }
      }

      // ëŒ€í™” ì•„ì´í…œ ìƒì„± (ë°±ì—…)
      if (serverEvent.type === 'conversation.item.created') {
        const item = serverEvent.item;
        if (item?.type === 'message' && item?.role === 'user') {
          console.log(`[${speaker}] ðŸ’¬ Conversation item:`, item);

          // content ë°°ì—´ì—ì„œ ì „ì‚¬ë³¸ ì¶”ì¶œ
          item.content?.forEach((content: any) => {
            if (content.type === 'input_audio' && content.transcript) {
              const entry: TranscriptEntry = {
                id: `transcript-${Date.now()}`,
                speaker,
                text: content.transcript.trim(),
                isFinal: true,
                timestamp: new Date(),
              };
              addTranscript(entry);
            }
          });
        }
      }

      // ì—ëŸ¬ ì²˜ë¦¬
      if (serverEvent.type === 'error') {
        console.error(`[${speaker}] âŒ API Error:`, serverEvent.error);
        setError(serverEvent.error?.message || 'Unknown error');
      }

    } catch (err) {
      console.error(`[${speaker}] Failed to parse event:`, err);
    }
  }, [speaker, addTranscript, updateInterim]);

  // WebRTC ì—°ê²° ì‹œìž‘
  const startConnection = useCallback(async () => {
    try {
      setError(null);
      setIsConnecting(true);
      console.log(`[${speaker}] ðŸš€ Starting Realtime connection...`);

      // 1. Ephemeral token ê°€ì ¸ì˜¤ê¸°
      console.log(`[${speaker}] ðŸ”‘ Fetching ephemeral token...`);
      const tokenResponse = await fetch('/api/realtime/session', {
        method: 'POST',
      });

      if (!tokenResponse.ok) {
        throw new Error('Failed to get session token');
      }

      const tokenData = await tokenResponse.json();
      const ephemeralKey = tokenData.value;

      if (!ephemeralKey) {
        throw new Error('No ephemeral key received');
      }

      console.log(`[${speaker}] âœ… Got ephemeral token`);

      // 2. RTCPeerConnection ìƒì„±
      const pc = new RTCPeerConnection();
      peerConnectionRef.current = pc;

      // 3. ì˜¤ë””ì˜¤ ìž¬ìƒ ì„¤ì •
      const audioElement = document.createElement('audio');
      audioElement.autoplay = true;
      audioElementRef.current = audioElement;

      pc.ontrack = (e) => {
        console.log(`[${speaker}] ðŸŽµ Received remote track`);
        if (audioElementRef.current) {
          audioElementRef.current.srcObject = e.streams[0];
        }
      };

      // 4. ë§ˆì´í¬ ìž…ë ¥ ì¶”ê°€
      console.log(`[${speaker}] ðŸŽ¤ Requesting microphone access...`);
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true,
          sampleRate: 24000,
        },
      });

      stream.getTracks().forEach(track => {
        pc.addTrack(track, stream);
      });

      console.log(`[${speaker}] âœ… Microphone added to peer connection`);

      // 5. Data channel ì„¤ì •
      const dc = pc.createDataChannel('oai-events');
      dataChannelRef.current = dc;

      dc.onopen = () => {
        console.log(`[${speaker}] ðŸ“¡ Data channel opened`);
        setIsConnected(true);
        setIsConnecting(false);
        setStreaming(true);

        // ðŸ”¥ ì„¸ì…˜ ì—…ë°ì´íŠ¸: ì‹¤ì‹œê°„ ì „ì‚¬ í™œì„±í™”
        const updateEvent = {
          type: 'session.update',
          session: {
            turn_detection: {
              type: 'server_vad',
              threshold: 0.5,
              prefix_padding_ms: 300,
              silence_duration_ms: 500,
            },
            input_audio_transcription: {
              model: 'whisper-1',
            },
          },
        };

        console.log(`[${speaker}] ðŸ“¤ Sending session.update:`, updateEvent);
        dc.send(JSON.stringify(updateEvent));
      };

      dc.onclose = () => {
        console.log(`[${speaker}] ðŸ“¡ Data channel closed`);
        setIsConnected(false);
        setStreaming(false);
      };

      dc.onerror = (e) => {
        console.error(`[${speaker}] âŒ Data channel error:`, e);
        setError('Data channel error');
      };

      dc.onmessage = handleServerEvent;

      // 6. SDP êµí™˜
      console.log(`[${speaker}] ðŸ¤ Creating offer...`);
      const offer = await pc.createOffer();
      await pc.setLocalDescription(offer);

      console.log(`[${speaker}] ðŸ“¤ Sending SDP to OpenAI...`);
      const sdpResponse = await fetch('https://api.openai.com/v1/realtime/calls', {
        method: 'POST',
        body: offer.sdp,
        headers: {
          Authorization: `Bearer ${ephemeralKey}`,
          'Content-Type': 'application/sdp',
        },
      });

      if (!sdpResponse.ok) {
        throw new Error('Failed to establish WebRTC connection');
      }

      const answerSdp = await sdpResponse.text();
      const answer: RTCSessionDescriptionInit = {
        type: 'answer',
        sdp: answerSdp,
      };

      await pc.setRemoteDescription(answer);

      console.log(`[${speaker}] âœ… WebRTC connection established`);

    } catch (err) {
      console.error(`[${speaker}] âŒ Connection failed:`, err);
      setError(err instanceof Error ? err.message : 'Failed to start connection');
      setIsConnecting(false);
      setIsConnected(false);

      if (peerConnectionRef.current) {
        peerConnectionRef.current.close();
        peerConnectionRef.current = null;
      }
    }
  }, [speaker, handleServerEvent, setStreaming]);

  // ì—°ê²° ì¢…ë£Œ
  const stopConnection = useCallback(() => {
    console.log(`[${speaker}] ðŸ›‘ Stopping connection...`);

    if (dataChannelRef.current) {
      dataChannelRef.current.close();
      dataChannelRef.current = null;
    }

    if (peerConnectionRef.current) {
      peerConnectionRef.current.close();
      peerConnectionRef.current = null;
    }

    if (audioElementRef.current) {
      audioElementRef.current.srcObject = null;
      audioElementRef.current = null;
    }

    setIsConnected(false);
    setIsConnecting(false);
    setStreaming(false);
  }, [speaker, setStreaming]);

  useEffect(() => {
    return () => {
      stopConnection();
    };
  }, [stopConnection]);

  return {
    isConnected,
    isConnecting,
    error,
    startConnection,
    stopConnection,
  };
}
